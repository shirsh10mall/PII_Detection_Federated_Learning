{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unidecode","metadata":{"_kg_hide-input":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-09-08T05:36:24.288352Z","iopub.execute_input":"2024-09-08T05:36:24.288741Z","iopub.status.idle":"2024-09-08T05:36:37.171955Z","shell.execute_reply.started":"2024-09-08T05:36:24.288705Z","shell.execute_reply":"2024-09-08T05:36:37.170964Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: unidecode in /opt/conda/lib/python3.10/site-packages (1.3.8)\n","output_type":"stream"}]},{"cell_type":"code","source":"from dotenv import load_dotenv\nimport os\nfrom pathlib import Path\nimport pandas as pd\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom unidecode import unidecode\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom loguru import logger\n\ntorch.cuda.memory._record_memory_history()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:37.173910Z","iopub.execute_input":"2024-09-08T05:36:37.174230Z","iopub.status.idle":"2024-09-08T05:36:40.339327Z","shell.execute_reply.started":"2024-09-08T05:36:37.174198Z","shell.execute_reply":"2024-09-08T05:36:40.338297Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"[W908 05:36:40.041943949 unwind.cpp:209] Warning: Unsupported unwinding pattern: Address not in range (function unwinderFor)\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_devices() -> list:\n    \"\"\"\n    Returns a list of available torch devices.\n    Prioritizes CUDA (GPU) if available, followed by MPS (Apple Silicon), \n    and defaults to CPU if neither are available.\n    \"\"\"\n    devices = []\n    \n    if torch.cuda.is_available():\n        # Add all available CUDA devices\n        for i in range(torch.cuda.device_count()):\n            device = torch.device(f\"cuda:{i}\")\n            devices.append(device)\n            logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(i)} (cuda:{i})\")\n    \n    elif torch.backends.mps.is_available():\n        # If CUDA is not available, add MPS device (Apple Silicon)\n        device = torch.device(\"mps\")\n        devices.append(device)\n        logger.info(\"Using MPS (Apple Silicon) device.\")\n    \n    else:\n        # If neither CUDA nor MPS are available, default to CPU\n        device = torch.device(\"cpu\")\n        devices.append(device)\n        logger.info(\"Using CPU device.\")\n\n    return devices","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:40.340547Z","iopub.execute_input":"2024-09-08T05:36:40.341051Z","iopub.status.idle":"2024-09-08T05:36:40.349008Z","shell.execute_reply.started":"2024-09-08T05:36:40.341015Z","shell.execute_reply":"2024-09-08T05:36:40.348106Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Config:\n    # Model Config\n    model_id = \"microsoft/deberta-v3-base\"\n    model_architecture_config = AutoConfig.from_pretrained(\n        model_id, output_hidden_states=True\n    )\n\n    # Training Config\n    batch_size = 4\n    max_length = 1024 * 2 + 256\n    num_workers = 2\n\n    # Hardware Config\n    torch_device = get_devices()\n\n    # Dataset\n    dataset_file_path = (\n        \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n    )\n    split_config = {\n        \"test_size\": 0.2,\n        \"shuffle\": True,\n        \"random_state\": 10,\n    }\n    sample_only = False\n    sample_size = 32\n\n    # Labels:\n    label2id = {\n        \"O\": 0,\n        \"B-EMAIL\": 1,\n        \"B-ID_NUM\": 2,\n        \"B-NAME_STUDENT\": 3,\n        \"B-PHONE_NUM\": 4,\n        \"B-STREET_ADDRESS\": 5,\n        \"B-URL_PERSONAL\": 6,\n        \"B-USERNAME\": 7,\n        \"I-ID_NUM\": 8,\n        \"I-NAME_STUDENT\": 9,\n        \"I-PHONE_NUM\": 10,\n        \"I-STREET_ADDRESS\": 11,\n        \"I-URL_PERSONAL\": 12,\n    }\n    id2label = {\n        \"0\": \"O\",\n        \"1\": \"B-EMAIL\",\n        \"2\": \"B-ID_NUM\",\n        \"3\": \"B-NAME_STUDENT\",\n        \"4\": \"B-PHONE_NUM\",\n        \"5\": \"B-STREET_ADDRESS\",\n        \"6\": \"B-URL_PERSONAL\",\n        \"7\": \"B-USERNAME\",\n        \"8\": \"I-ID_NUM\",\n        \"9\": \"I-NAME_STUDENT\",\n        \"10\": \"I-PHONE_NUM\",\n        \"11\": \"I-STREET_ADDRESS\",\n        \"12\": \"I-URL_PERSONAL\",\n    }\n    num_labels = len(label2id)\n\n\nprint(\"torch_device: \", Config.torch_device)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:40.351697Z","iopub.execute_input":"2024-09-08T05:36:40.352448Z","iopub.status.idle":"2024-09-08T05:36:40.744151Z","shell.execute_reply.started":"2024-09-08T05:36:40.352402Z","shell.execute_reply":"2024-09-08T05:36:40.743173Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[32m2024-09-08 05:36:40.739\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_devices\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1mUsing CUDA device: Tesla P100-PCIE-16GB (cuda:0)\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"torch_device:  [device(type='cuda', index=0)]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    Config.model_id,\n    use_fast=True,  # to avoid warnings\n    clean_up_tokenization_spaces=False,  # to avoid warnings\n    max_length=Config.max_length,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:40.745412Z","iopub.execute_input":"2024-09-08T05:36:40.745725Z","iopub.status.idle":"2024-09-08T05:36:42.177073Z","shell.execute_reply.started":"2024-09-08T05:36:40.745693Z","shell.execute_reply":"2024-09-08T05:36:42.176024Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:551: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_json(Config.dataset_file_path)\n\nif Config.sample_only:\n    df = df[0 : Config.sample_size]\n\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:42.178422Z","iopub.execute_input":"2024-09-08T05:36:42.178946Z","iopub.status.idle":"2024-09-08T05:36:43.818443Z","shell.execute_reply.started":"2024-09-08T05:36:42.178903Z","shell.execute_reply":"2024-09-08T05:36:43.817382Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   document                                          full_text  \\\n0         7  Design Thinking for innovation reflexion-Avril...   \n1        10  Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...   \n2        16  Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...   \n3        20  Design Thinking for Innovation\\n\\nSindy Samaca...   \n4        56  Assignment:  Visualization Reflection  Submitt...   \n\n                                              tokens  \\\n0  [Design, Thinking, for, innovation, reflexion,...   \n1  [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n2  [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n3  [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n4  [Assignment, :,   , Visualization,  , Reflecti...   \n\n                                 trailing_whitespace  \\\n0  [True, True, True, True, False, False, True, F...   \n1  [True, False, False, True, True, False, False,...   \n2  [True, False, False, True, True, False, False,...   \n3  [True, True, True, False, False, True, False, ...   \n4  [False, False, False, False, False, False, Fal...   \n\n                                              labels  \n0  [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n1  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n2  [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n3  [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n4  [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>full_text</th>\n      <th>tokens</th>\n      <th>trailing_whitespace</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>Design Thinking for innovation reflexion-Avril...</td>\n      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n      <td>[True, True, True, True, False, False, True, F...</td>\n      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>Diego Estrada\\n\\nDesign Thinking Assignment\\n\\...</td>\n      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n      <td>[True, False, False, True, True, False, False,...</td>\n      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16</td>\n      <td>Reporting process\\n\\nby Gilberto Gamboa\\n\\nCha...</td>\n      <td>[Reporting, process, \\n\\n, by, Gilberto, Gambo...</td>\n      <td>[True, False, False, True, True, False, False,...</td>\n      <td>[O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20</td>\n      <td>Design Thinking for Innovation\\n\\nSindy Samaca...</td>\n      <td>[Design, Thinking, for, Innovation, \\n\\n, Sind...</td>\n      <td>[True, True, True, False, False, True, False, ...</td>\n      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>56</td>\n      <td>Assignment:  Visualization Reflection  Submitt...</td>\n      <td>[Assignment, :,   , Visualization,  , Reflecti...</td>\n      <td>[False, False, False, False, False, False, Fal...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def replace_space(tokens_list):\n    return [\"[SPACE]\" if x.isspace() else x for x in tokens_list]\n\n\ndef get_tokenized_tokens_length(text):\n    return len(\n        tokenizer(text, return_attention_mask=False, return_token_type_ids=False)[\n            \"input_ids\"\n        ]\n    )\n\n\ndef data_preprocessing(df):\n    df[\"tokens\"] = df[\"tokens\"].apply(replace_space)\n\n    df[\"tokenized_tokens_length\"] = df[\"full_text\"].apply(\n        lambda text: get_tokenized_tokens_length(text)\n    )\n    df = df.sort_values(by=\"tokenized_tokens_length\", ascending=True).reset_index(\n        drop=True\n    )\n\n    return df\n\n\ndf = data_preprocessing(df=df)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:43.819943Z","iopub.execute_input":"2024-09-08T05:36:43.820795Z","iopub.status.idle":"2024-09-08T05:36:58.247437Z","shell.execute_reply.started":"2024-09-08T05:36:43.820743Z","shell.execute_reply":"2024-09-08T05:36:58.246381Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      document                                          full_text  \\\n0        13147  Think Twice and Make a Wise\\n\\nConcept Mapping...   \n1        19614  Who are your target customers?\\n\\nPatients hav...   \n2        20900  Mind Mapping  selection:\\n\\nMind mapping is us...   \n3        11469  Construct Your Problems\\n\\nVisualization is on...   \n4        18959  Reflection writing rubric it represents throug...   \n...        ...                                                ...   \n6802      7745  Luis Gonzales  Savitribai Phule Pune Universit...   \n6803      9188  Design Thinking in Career Development and Coun...   \n6804     10078  Overcoming Barriers - The Story of the Movie a...   \n6805     21720  In this assignment, a reflective report will b...   \n6806     14267  MIAMISBURG, Ohio, Dec. 10, 2019 /PRNewswire/ -...   \n\n                                                 tokens  \\\n0     [Think, Twice, and, Make, a, Wise, [SPACE], Co...   \n1     [Who, are, your, target, customers, ?, [SPACE]...   \n2     [Mind, Mapping, [SPACE], selection, :, [SPACE]...   \n3     [Construct, Your, Problems, [SPACE], Visualiza...   \n4     [Reflection, writing, rubric, it, represents, ...   \n...                                                 ...   \n6802  [Luis, Gonzales, [SPACE], Savitribai, Phule, P...   \n6803  [Design, Thinking, in, Career, Development, an...   \n6804  [Overcoming, Barriers, -, The, Story, of, the,...   \n6805  [In, this, assignment, ,, a, reflective, repor...   \n6806  [MIAMISBURG, ,, Ohio, ,, Dec., 10, ,, 2019, /P...   \n\n                                    trailing_whitespace  \\\n0     [True, True, True, True, True, False, False, T...   \n1     [True, True, True, True, False, False, False, ...   \n2     [True, True, False, False, False, False, True,...   \n3     [True, True, False, False, True, True, True, T...   \n4     [True, True, True, True, True, True, False, Tr...   \n...                                                 ...   \n6802  [True, True, False, True, True, True, True, Fa...   \n6803  [True, True, True, True, True, True, True, Fal...   \n6804  [True, True, True, True, True, True, True, Tru...   \n6805  [True, True, False, True, True, True, True, Tr...   \n6806  [False, True, False, True, True, False, True, ...   \n\n                                                 labels  \\\n0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n3     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n...                                                 ...   \n6802  [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...   \n6803  [O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-NAM...   \n6804  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n6805  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n6806  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n\n      tokenized_tokens_length  \n0                          67  \n1                          68  \n2                          76  \n3                          81  \n4                          84  \n...                       ...  \n6802                     2560  \n6803                     2749  \n6804                     2831  \n6805                     2900  \n6806                     3076  \n\n[6807 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>document</th>\n      <th>full_text</th>\n      <th>tokens</th>\n      <th>trailing_whitespace</th>\n      <th>labels</th>\n      <th>tokenized_tokens_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13147</td>\n      <td>Think Twice and Make a Wise\\n\\nConcept Mapping...</td>\n      <td>[Think, Twice, and, Make, a, Wise, [SPACE], Co...</td>\n      <td>[True, True, True, True, True, False, False, T...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19614</td>\n      <td>Who are your target customers?\\n\\nPatients hav...</td>\n      <td>[Who, are, your, target, customers, ?, [SPACE]...</td>\n      <td>[True, True, True, True, False, False, False, ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20900</td>\n      <td>Mind Mapping  selection:\\n\\nMind mapping is us...</td>\n      <td>[Mind, Mapping, [SPACE], selection, :, [SPACE]...</td>\n      <td>[True, True, False, False, False, False, True,...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>76</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11469</td>\n      <td>Construct Your Problems\\n\\nVisualization is on...</td>\n      <td>[Construct, Your, Problems, [SPACE], Visualiza...</td>\n      <td>[True, True, False, False, True, True, True, T...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>81</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18959</td>\n      <td>Reflection writing rubric it represents throug...</td>\n      <td>[Reflection, writing, rubric, it, represents, ...</td>\n      <td>[True, True, True, True, True, True, False, Tr...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6802</th>\n      <td>7745</td>\n      <td>Luis Gonzales  Savitribai Phule Pune Universit...</td>\n      <td>[Luis, Gonzales, [SPACE], Savitribai, Phule, P...</td>\n      <td>[True, True, False, True, True, True, True, Fa...</td>\n      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n      <td>2560</td>\n    </tr>\n    <tr>\n      <th>6803</th>\n      <td>9188</td>\n      <td>Design Thinking in Career Development and Coun...</td>\n      <td>[Design, Thinking, in, Career, Development, an...</td>\n      <td>[True, True, True, True, True, True, True, Fal...</td>\n      <td>[O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-NAM...</td>\n      <td>2749</td>\n    </tr>\n    <tr>\n      <th>6804</th>\n      <td>10078</td>\n      <td>Overcoming Barriers - The Story of the Movie a...</td>\n      <td>[Overcoming, Barriers, -, The, Story, of, the,...</td>\n      <td>[True, True, True, True, True, True, True, Tru...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>2831</td>\n    </tr>\n    <tr>\n      <th>6805</th>\n      <td>21720</td>\n      <td>In this assignment, a reflective report will b...</td>\n      <td>[In, this, assignment, ,, a, reflective, repor...</td>\n      <td>[True, True, False, True, True, True, True, Tr...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>2900</td>\n    </tr>\n    <tr>\n      <th>6806</th>\n      <td>14267</td>\n      <td>MIAMISBURG, Ohio, Dec. 10, 2019 /PRNewswire/ -...</td>\n      <td>[MIAMISBURG, ,, Ohio, ,, Dec., 10, ,, 2019, /P...</td>\n      <td>[False, True, False, True, True, False, True, ...</td>\n      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n      <td>3076</td>\n    </tr>\n  </tbody>\n</table>\n<p>6807 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = df[df[\"tokenized_tokens_length\"]<256]\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:58.248775Z","iopub.execute_input":"2024-09-08T05:36:58.249473Z","iopub.status.idle":"2024-09-08T05:36:58.257159Z","shell.execute_reply.started":"2024-09-08T05:36:58.249425Z","shell.execute_reply":"2024-09-08T05:36:58.256188Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(242, 6)"},"metadata":{}}]},{"cell_type":"code","source":"if not Config.sample_only:\n    Config.split_config[\"stratify\"] = pd.cut(\n        df[\"tokenized_tokens_length\"], bins=10, labels=False\n    )\n\ntrain_df, test_df = train_test_split(df, **Config.split_config)\ntrain_df.reset_index(inplace=True)\ntrain_df = train_df.sort_values(by=\"tokenized_tokens_length\", ascending=True).reset_index(drop=True)\ntest_df = test_df.sort_values(by=\"tokenized_tokens_length\", ascending=True).reset_index(drop=True)\ntest_df.reset_index(inplace=True)\ntrain_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:58.258358Z","iopub.execute_input":"2024-09-08T05:36:58.258681Z","iopub.status.idle":"2024-09-08T05:36:58.276932Z","shell.execute_reply.started":"2024-09-08T05:36:58.258649Z","shell.execute_reply":"2024-09-08T05:36:58.275938Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((193, 7), (49, 7))"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_input(row, tokenizer):\n    processed_text_tokens_list = []\n    char_map = []\n    label_char_map = {}\n\n    for index in range(len(row[\"tokens\"])):\n        token = unidecode(row[\"tokens\"][index])\n\n        whitespace = row[\"trailing_whitespace\"][index]\n        label = row[\"labels\"][index]\n\n        processed_text_tokens_list.append(token)\n        char_map.extend([index] * len(token))\n\n        label_char_map[index] = label\n\n        if whitespace:\n            processed_text_tokens_list.append(\" \")\n            char_map.append(-1)\n\n    # Now, we tokenize the concatenated 'text' and return offsets mappings along with 'char_map'.\n    processed_text = \"\".join(processed_text_tokens_list)\n    tokenized = tokenizer(\n        processed_text,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=Config.max_length,\n    )\n\n    length = len(tokenized.input_ids)\n\n    tokenized_info = {\n        **tokenized,\n        \"processed_text\": processed_text,\n        \"length\": length,\n        \"char_map\": char_map,  # Now includes mapping to original tokens\n        \"label_char_map\": label_char_map,\n    }\n    return tokenized_info\n\n\ndef get_labels(tokenized_info):\n    label_list = []\n    offset_map = tokenized_info[\"offset_mapping\"]\n    for index, offset_map_item in enumerate(offset_map):\n        if offset_map_item == (0, 0):\n            label_list.extend([\"Start_End\"])\n            continue\n\n        char_map_item = tokenized_info[\"char_map\"][\n            offset_map_item[0] : offset_map_item[1]\n        ]\n        char_map_item_filtered = [element for element in char_map_item if element != -1]\n\n        label_item = set(\n            [\n                tokenized_info[\"label_char_map\"][element]\n                for element in char_map_item_filtered\n            ]\n        )\n\n        if len(label_item) != 1:\n            if tokenized_info[\"input_ids\"][index] in [507]:\n                label_item = \"O\"\n\n            else:\n                raise Exception(\n                    \"\\n\"\n                    f\"Token ID: {tokenized_info['input_ids'][index]}\\n\"\n                    f\"Token: {tokenizer.decode(tokenized_info['input_ids'][index])}\\n\"\n                    f\"Offset: {offset_map_item}\\n\"\n                    f\"Text: {tokenized_info['processed_text'][ offset_map_item[0] : offset_map_item[1] ]}\\n\"\n                    f\"Character Map: {char_map_item}\\n\"\n                    f\"Filtered Character Map {char_map_item_filtered}\\n\"\n                    f\"Labels: {label_item}\"\n                )\n\n        label_list.extend(list(label_item))\n\n    if len(label_list) != len(tokenized_info[\"input_ids\"]):\n        raise Exception(\"Error: Size of label_list and input_ids are not same.\")\n    return label_list","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:58.280638Z","iopub.execute_input":"2024-09-08T05:36:58.281020Z","iopub.status.idle":"2024-09-08T05:36:58.362752Z","shell.execute_reply.started":"2024-09-08T05:36:58.280960Z","shell.execute_reply":"2024-09-08T05:36:58.361828Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Check - Test - Dataset\nfor index in tqdm(train_df.index):\n    tokenized_info = prepare_input(train_df.iloc[index], tokenizer)\n    label_item = get_labels(tokenized_info)\n\nprint(\"Awesome - Everything is fine\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:58.364127Z","iopub.execute_input":"2024-09-08T05:36:58.365104Z","iopub.status.idle":"2024-09-08T05:36:59.292926Z","shell.execute_reply.started":"2024-09-08T05:36:58.365057Z","shell.execute_reply":"2024-09-08T05:36:59.292020Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 193/193 [00:00<00:00, 212.22it/s]","output_type":"stream"},{"name":"stdout","text":"Awesome - Everything is fine\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"class PII_Dataset(Dataset):\n    def __init__(self, tokenizer, df):\n        self.tokenizer = tokenizer\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index].to_dict()\n        row.pop(\"tokenized_tokens_length\")\n\n        tokenized_info = prepare_input(self.df.iloc[index], tokenizer)\n        label_item = get_labels(tokenized_info)\n        tokenized_info[\"document_id\"] = row.pop(\"document\")\n        tokenized_info[\"labels\"] = label_item\n        tokenized_info[\"label_ids\"] = [\n            0 if item == \"Start_End\" else Config.label2id[item] for item in label_item\n        ]\n\n        if len(tokenized_info[\"label_ids\"]) != len(tokenized_info[\"input_ids\"]):\n            raise Exception(\n                f\"Error in tokenized_info - length of lavel_ids and input_ids are not same: {tokenized_info}\"\n            )\n        return tokenized_info","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:59.294304Z","iopub.execute_input":"2024-09-08T05:36:59.294631Z","iopub.status.idle":"2024-09-08T05:36:59.303080Z","shell.execute_reply.started":"2024-09-08T05:36:59.294597Z","shell.execute_reply":"2024-09-08T05:36:59.301857Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        # List of keys to extract from each sample\n        keys = [\n            \"document_id\",\n            \"input_ids\",\n            \"token_type_ids\",\n            \"attention_mask\",\n#             \"offset_mapping\",\n#             \"processed_text\",\n#             \"length\",\n#             \"char_map\",\n#             \"label_char_map\",\n#             \"labels\",\n            \"label_ids\",\n        ]\n\n        # Populate the output dictionary using a loop\n        output = {key: [sample[key] for sample in batch] for key in keys}\n\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n\n        # Add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [\n                list(s) + (batch_max - len(s)) * [self.tokenizer.pad_token_id]\n                for s in output[\"input_ids\"]\n            ]\n            output[\"attention_mask\"] = [\n                list(s) + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]\n            ]\n            output[\"token_type_ids\"] = [\n                list(s) + (batch_max - len(s)) * [0] for s in output[\"token_type_ids\"]\n            ]\n#             output[\"offset_mapping\"] = [\n#                 list(s) + (batch_max - len(s)) * [(0, 0)]\n#                 for s in output[\"offset_mapping\"]\n#             ]\n            output[\"label_ids\"] = [\n                list(s) + (batch_max - len(s)) * [0] for s in output[\"label_ids\"]\n            ]\n\n        # Convert to tensors and move to the specified device\n        keys = [\"document_id\", \"input_ids\", \"attention_mask\", \"token_type_ids\", \"label_ids\"]\n        for key in keys:\n            output[key] = torch.tensor(output[key], dtype=torch.long) # .to(Config.torch_device[0])\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:59.304532Z","iopub.execute_input":"2024-09-08T05:36:59.305264Z","iopub.status.idle":"2024-09-08T05:36:59.316247Z","shell.execute_reply.started":"2024-09-08T05:36:59.305219Z","shell.execute_reply":"2024-09-08T05:36:59.315356Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_dataset = PII_Dataset(tokenizer, df=train_df)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=Config.batch_size,\n    shuffle=True,\n    collate_fn=Collate(tokenizer),\n    # num_workers=Config.num_workers,\n    pin_memory=True,\n    drop_last=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:59.317380Z","iopub.execute_input":"2024-09-08T05:36:59.317664Z","iopub.status.idle":"2024-09-08T05:36:59.332629Z","shell.execute_reply.started":"2024-09-08T05:36:59.317634Z","shell.execute_reply":"2024-09-08T05:36:59.331791Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Checking data loader\nfor item in tqdm(train_loader):\n    if len(item[\"input_ids\"]) != len(item[\"label_ids\"]):\n        raise Exception(\n            \"Error: length of input_ids and label_ids after padding are not same.\"\n        )\n    pass\n\nitem.keys()","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:36:59.333842Z","iopub.execute_input":"2024-09-08T05:36:59.334402Z","iopub.status.idle":"2024-09-08T05:37:00.426162Z","shell.execute_reply.started":"2024-09-08T05:36:59.334368Z","shell.execute_reply":"2024-09-08T05:37:00.425251Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 49/49 [00:01<00:00, 45.64it/s]\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"dict_keys(['document_id', 'input_ids', 'token_type_ids', 'attention_mask', 'label_ids'])"},"metadata":{}}]},{"cell_type":"code","source":"class PIIDetectionModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.num_labels = Config.model_architecture_config.num_labels\n\n        self.model = AutoModel.from_pretrained(\n            Config.model_id,\n            ignore_mismatched_sizes=True,\n            config=Config.model_architecture_config,\n            # torch_dtype = \"auto\"\n        )\n        self.model.gradient_checkpointing_enable()\n        self.model.resize_token_embeddings(len(tokenizer))\n        self.dropout = torch.nn.Dropout(\n            Config.model_architecture_config.hidden_dropout_prob\n        )\n\n        self.bilstm = torch.nn.LSTM(\n            Config.model_architecture_config.hidden_size,\n            (Config.model_architecture_config.hidden_size) // 2,\n            num_layers=2,\n            dropout=Config.model_architecture_config.hidden_dropout_prob,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.gru = torch.nn.GRU(\n            Config.model_architecture_config.hidden_size,\n            Config.model_architecture_config.hidden_size // 2,\n            num_layers=2,\n            dropout=Config.model_architecture_config.hidden_dropout_prob,\n            batch_first=True,\n            bidirectional=True,\n        )\n        \n        self.lstm_gru_balance_weight = torch.nn.Parameter(\n            torch.tensor(0.5), requires_grad=False\n        )\n\n        self.fc = torch.nn.Linear(\n            Config.model_architecture_config.hidden_size, Config.num_labels\n        )\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )  # returns ['last_hidden_state', 'hidden_states']\n        sequence_output = output[0]\n        sequence_output = self.dropout(sequence_output)\n        lstm_output, hc = self.bilstm(sequence_output)\n        gru_output, _ = self.gru(sequence_output)\n\n        rnn_output = (\n            self.lstm_gru_balance_weight * lstm_output\n            + (1 - self.lstm_gru_balance_weight) * gru_output\n        )\n        logits = self.fc(rnn_output)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:38:49.056852Z","iopub.execute_input":"2024-09-08T05:38:49.057758Z","iopub.status.idle":"2024-09-08T05:38:49.068814Z","shell.execute_reply.started":"2024-09-08T05:38:49.057714Z","shell.execute_reply":"2024-09-08T05:38:49.067776Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def convert_logits_to_labels(logits: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Convert logits into predicted labels for token classification.\n    \"\"\"\n    probabilities = torch.softmax(logits, dim=-1)\n    predicted_labels = torch.argmax(probabilities, dim=-1)\n    return predicted_labels\n\n\ndef calculate_loss(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n    # TODO: Loss function excluding CLS, Start and End tokens: https://chatgpt.com/c/66dbbd83-4930-8007-b247-0d73fc2ee9af\n    \"\"\"\n    Calculate the cross-entropy loss for token classification using raw logits.\n    \"\"\"\n    loss_fn = torch.nn.CrossEntropyLoss()\n    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n    return loss\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:38:49.341135Z","iopub.execute_input":"2024-09-08T05:38:49.341513Z","iopub.status.idle":"2024-09-08T05:38:49.348246Z","shell.execute_reply.started":"2024-09-08T05:38:49.341478Z","shell.execute_reply":"2024-09-08T05:38:49.347133Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = PIIDetectionModel().to(Config.torch_device[0])\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:38:49.849268Z","iopub.execute_input":"2024-09-08T05:38:49.849982Z","iopub.status.idle":"2024-09-08T05:38:54.079568Z","shell.execute_reply.started":"2024-09-08T05:38:49.849942Z","shell.execute_reply":"2024-09-08T05:38:54.078646Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"PIIDetectionModel(\n  (model): DebertaV2Model(\n    (embeddings): DebertaV2Embeddings(\n      (word_embeddings): Embedding(128001, 768, padding_idx=0)\n      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n      (dropout): StableDropout()\n    )\n    (encoder): DebertaV2Encoder(\n      (layer): ModuleList(\n        (0-11): 12 x DebertaV2Layer(\n          (attention): DebertaV2Attention(\n            (self): DisentangledSelfAttention(\n              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n              (pos_dropout): StableDropout()\n              (dropout): StableDropout()\n            )\n            (output): DebertaV2SelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n              (dropout): StableDropout()\n            )\n          )\n          (intermediate): DebertaV2Intermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaV2Output(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n            (dropout): StableDropout()\n          )\n        )\n      )\n      (rel_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (bilstm): LSTM(768, 384, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n  (gru): GRU(768, 384, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n  (fc): Linear(in_features=768, out_features=13, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef count_trainable_parameters(model: nn.Module) -> int:\n    \"\"\"\n    Count the number of trainable parameters in a PyTorch model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef estimate_parameter_memory(model: nn.Module, dtype=torch.float32) -> float:\n    \"\"\"\n    Estimate the memory required to store the parameters of a PyTorch model. The estimated memory in megabytes (MB).\n    \"\"\"\n    num_params = count_trainable_parameters(model)\n    \n    # Memory per element in bytes, e.g., 4 bytes for float32, 2 bytes for float16\n    bytes_per_element = torch.finfo(dtype).bits // 8\n    \n    # Total memory in bytes\n    total_memory_bytes = num_params * bytes_per_element\n    \n    # Convert to megabytes (MB)\n    total_memory_mb = total_memory_bytes / (1024 ** 2)\n    \n    return num_params, total_memory_mb","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:38:58.276707Z","iopub.execute_input":"2024-09-08T05:38:58.277622Z","iopub.status.idle":"2024-09-08T05:38:58.284375Z","shell.execute_reply.started":"2024-09-08T05:38:58.277579Z","shell.execute_reply":"2024-09-08T05:38:58.283338Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"num_params, total_memory_mb = estimate_parameter_memory(model.model)\nprint(f\"Number of trainable parameters in GRU: {num_params}\")\nprint(f\"Estimated memory for GRU parameters: {total_memory_mb:.3f} MB\\n\")\n\n\nnum_params, total_memory_mb = estimate_parameter_memory(model.bilstm)\nprint(f\"Number of trainable parameters in GRU: {num_params}\")\nprint(f\"Estimated memory for GRU parameters: {total_memory_mb:.3f} MB\\n\")\n\n\nnum_params, total_memory_mb = estimate_parameter_memory(model.gru)\nprint(f\"Number of trainable parameters in GRU: {num_params}\")\nprint(f\"Estimated memory for GRU parameters: {total_memory_mb:.3f} MB\\n\")\n\n\nnum_params, total_memory_mb = estimate_parameter_memory(model.fc)\nprint(f\"Number of trainable parameters in GRU: {num_params}\")\nprint(f\"Estimated memory for GRU parameters: {total_memory_mb:.3f} MB\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:38:59.002570Z","iopub.execute_input":"2024-09-08T05:38:59.002993Z","iopub.status.idle":"2024-09-08T05:38:59.012055Z","shell.execute_reply.started":"2024-09-08T05:38:59.002953Z","shell.execute_reply":"2024-09-08T05:38:59.011045Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Number of trainable parameters in GRU: 183755520\nEstimated memory for GRU parameters: 700.972 MB\n\nNumber of trainable parameters in GRU: 7090176\nEstimated memory for GRU parameters: 27.047 MB\n\nNumber of trainable parameters in GRU: 5317632\nEstimated memory for GRU parameters: 20.285 MB\n\nNumber of trainable parameters in GRU: 9997\nEstimated memory for GRU parameters: 0.038 MB\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Checking data loader\n# for item in tqdm(train_loader):\n#     break\n\n\n# # Chechking models forward pass\n# model_output = model.forward(\n#     input_ids=item[\"input_ids\"].to(Config.torch_device[0]),\n#     attention_mask=item[\"attention_mask\"].to(Config.torch_device[0]),\n#     token_type_ids=item[\"token_type_ids\"].to(Config.torch_device[0]),\n# )\n\n# # from torchviz import make_dot\n# # make_dot(model_output.last_hidden_state.mean(), params=dict(custom_model.named_parameters()))\n\n\n# print(\n#     \"logits_shape: \",\n#     model_output.shape,\n#     \"\\ninput_ids_shape:\",\n#     item[\"input_ids\"].shape,\n#     \"\\noutput_labels_shape: \",\n#     item[\"label_ids\"].shape,\n# )\n\n# # Checking loss function\n# loss, predicted_labels = calculate_loss(logits=model_output, labels=item[\"label_ids\"].to(Config.torch_device[0]))\n# loss","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:38:59.448695Z","iopub.execute_input":"2024-09-08T05:38:59.449625Z","iopub.status.idle":"2024-09-08T05:38:59.454200Z","shell.execute_reply.started":"2024-09-08T05:38:59.449582Z","shell.execute_reply":"2024-09-08T05:38:59.453137Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from torch.amp import autocast, GradScaler\n\nlearning_rate = 5e-5\nepochs = 1\naccumulation_steps = 4  # Number of steps to accumulate gradients before an update\n\nscaler = GradScaler()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n\n    # Reset gradients at the start of the epoch\n    optimizer.zero_grad()\n\n    for step, batch in enumerate(progress_bar):\n        input_ids = batch[\"input_ids\"].to(Config.torch_device[0])\n        attention_mask = batch[\"attention_mask\"].to(Config.torch_device[0])\n        token_type_ids = batch[\"token_type_ids\"].to(Config.torch_device[0])\n        labels = batch[\"label_ids\"].to(Config.torch_device[0])\n\n        print(f\"Token Length: {len(input_ids[0])}\")\n\n        with autocast(device_type=Config.torch_device[0].type):  # Mixed precision\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = calculate_loss(logits=logits, labels=labels)\n            loss = loss / accumulation_steps  # Normalize loss over accumulation steps\n\n        scaler.scale(loss).backward()  # Scale loss and accumulate gradients\n\n        # Step the optimizer every `accumulation_steps`\n        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n            scaler.step(optimizer)  # Update model weights\n            scaler.update()  # Update the scaler for next batch\n            optimizer.zero_grad()  # Reset gradients after each update\n\n        # Detach inputs and labels to avoid memory accumulation\n        input_ids.detach()\n        attention_mask.detach()\n        token_type_ids.detach()\n        labels.detach()\n\n        print(\"loss: \", loss.item() * accumulation_steps)  # Adjust the loss back\n\n        total_loss += loss.item() * accumulation_steps  # Accumulated loss for the batch\n        progress_bar.set_postfix({\"loss\": f\"{loss.item() * accumulation_steps:.4f}\"})\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-08T05:39:00.370746Z","iopub.execute_input":"2024-09-08T05:39:00.371172Z","iopub.status.idle":"2024-09-08T05:39:21.885796Z","shell.execute_reply.started":"2024-09-08T05:39:00.371134Z","shell.execute_reply":"2024-09-08T05:39:21.884808Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"\nEpoch 1/1\n","output_type":"stream"},{"name":"stderr","text":"Training:   0%|          | 0/49 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Token Length: 350\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\nTraining:   2%|▏         | 1/49 [00:00<00:33,  1.44it/s, loss=2.6371]","output_type":"stream"},{"name":"stdout","text":"loss:  2.6371052265167236\nToken Length: 319\n","output_type":"stream"},{"name":"stderr","text":"Training:   4%|▍         | 2/49 [00:01<00:25,  1.87it/s, loss=2.6248]","output_type":"stream"},{"name":"stdout","text":"loss:  2.6247506141662598\nToken Length: 276\n","output_type":"stream"},{"name":"stderr","text":"Training:   6%|▌         | 3/49 [00:01<00:21,  2.16it/s, loss=2.6017]","output_type":"stream"},{"name":"stdout","text":"loss:  2.6016509532928467\nToken Length: 324\n","output_type":"stream"},{"name":"stderr","text":"Training:   8%|▊         | 4/49 [00:02<00:22,  2.04it/s, loss=2.6213]","output_type":"stream"},{"name":"stdout","text":"loss:  2.6212730407714844\nToken Length: 220\n","output_type":"stream"},{"name":"stderr","text":"Training:  10%|█         | 5/49 [00:02<00:18,  2.38it/s, loss=2.2671]","output_type":"stream"},{"name":"stdout","text":"loss:  2.2671186923980713\nToken Length: 320\n","output_type":"stream"},{"name":"stderr","text":"Training:  12%|█▏        | 6/49 [00:02<00:18,  2.34it/s, loss=2.2679]","output_type":"stream"},{"name":"stdout","text":"loss:  2.2679474353790283\nToken Length: 299\n","output_type":"stream"},{"name":"stderr","text":"Training:  14%|█▍        | 7/49 [00:03<00:17,  2.38it/s, loss=2.2789]","output_type":"stream"},{"name":"stdout","text":"loss:  2.2789130210876465\nToken Length: 317\n","output_type":"stream"},{"name":"stderr","text":"Training:  16%|█▋        | 8/49 [00:03<00:17,  2.32it/s, loss=2.2697]","output_type":"stream"},{"name":"stdout","text":"loss:  2.269651412963867\nToken Length: 292\n","output_type":"stream"},{"name":"stderr","text":"Training:  18%|█▊        | 9/49 [00:04<00:16,  2.36it/s, loss=1.9707]","output_type":"stream"},{"name":"stdout","text":"loss:  1.9706964492797852\nToken Length: 306\n","output_type":"stream"},{"name":"stderr","text":"Training:  20%|██        | 10/49 [00:04<00:16,  2.37it/s, loss=1.9490]","output_type":"stream"},{"name":"stdout","text":"loss:  1.948979377746582\nToken Length: 304\n","output_type":"stream"},{"name":"stderr","text":"Training:  22%|██▏       | 11/49 [00:04<00:15,  2.38it/s, loss=1.9792]","output_type":"stream"},{"name":"stdout","text":"loss:  1.9791597127914429\nToken Length: 302\n","output_type":"stream"},{"name":"stderr","text":"Training:  24%|██▍       | 12/49 [00:05<00:15,  2.34it/s, loss=2.0036]","output_type":"stream"},{"name":"stdout","text":"loss:  2.0036354064941406\nToken Length: 301\n","output_type":"stream"},{"name":"stderr","text":"Training:  27%|██▋       | 13/49 [00:05<00:15,  2.38it/s, loss=1.6854]","output_type":"stream"},{"name":"stdout","text":"loss:  1.6853837966918945\nToken Length: 294\n","output_type":"stream"},{"name":"stderr","text":"Training:  29%|██▊       | 14/49 [00:06<00:14,  2.41it/s, loss=1.7192]","output_type":"stream"},{"name":"stdout","text":"loss:  1.7191652059555054\nToken Length: 342\n","output_type":"stream"},{"name":"stderr","text":"Training:  31%|███       | 15/49 [00:06<00:14,  2.34it/s, loss=1.6837]","output_type":"stream"},{"name":"stdout","text":"loss:  1.6836830377578735\nToken Length: 275\n","output_type":"stream"},{"name":"stderr","text":"Training:  33%|███▎      | 16/49 [00:06<00:13,  2.38it/s, loss=1.7093]","output_type":"stream"},{"name":"stdout","text":"loss:  1.709290623664856\nToken Length: 325\n","output_type":"stream"},{"name":"stderr","text":"Training:  35%|███▍      | 17/49 [00:07<00:13,  2.34it/s, loss=1.4141]","output_type":"stream"},{"name":"stdout","text":"loss:  1.414137601852417\nToken Length: 280\n","output_type":"stream"},{"name":"stderr","text":"Training:  37%|███▋      | 18/49 [00:07<00:12,  2.42it/s, loss=1.4125]","output_type":"stream"},{"name":"stdout","text":"loss:  1.412506103515625\nToken Length: 291\n","output_type":"stream"},{"name":"stderr","text":"Training:  39%|███▉      | 19/49 [00:08<00:12,  2.44it/s, loss=1.3809]","output_type":"stream"},{"name":"stdout","text":"loss:  1.3808770179748535\nToken Length: 283\n","output_type":"stream"},{"name":"stderr","text":"Training:  41%|████      | 20/49 [00:08<00:12,  2.42it/s, loss=1.3822]","output_type":"stream"},{"name":"stdout","text":"loss:  1.3821861743927002\nToken Length: 335\n","output_type":"stream"},{"name":"stderr","text":"Training:  43%|████▎     | 21/49 [00:09<00:11,  2.38it/s, loss=1.1420]","output_type":"stream"},{"name":"stdout","text":"loss:  1.1419973373413086\nToken Length: 657\n","output_type":"stream"},{"name":"stderr","text":"Training:  45%|████▍     | 22/49 [00:09<00:15,  1.75it/s, loss=1.2019]","output_type":"stream"},{"name":"stdout","text":"loss:  1.2018520832061768\nToken Length: 322\n","output_type":"stream"},{"name":"stderr","text":"Training:  47%|████▋     | 23/49 [00:10<00:13,  1.89it/s, loss=1.1519]","output_type":"stream"},{"name":"stdout","text":"loss:  1.1519324779510498\nToken Length: 319\n","output_type":"stream"},{"name":"stderr","text":"Training:  49%|████▉     | 24/49 [00:10<00:12,  1.95it/s, loss=1.0967]","output_type":"stream"},{"name":"stdout","text":"loss:  1.0967382192611694\nToken Length: 330\n","output_type":"stream"},{"name":"stderr","text":"Training:  51%|█████     | 25/49 [00:11<00:11,  2.02it/s, loss=0.8952]","output_type":"stream"},{"name":"stdout","text":"loss:  0.8951634168624878\nToken Length: 280\n","output_type":"stream"},{"name":"stderr","text":"Training:  53%|█████▎    | 26/49 [00:11<00:10,  2.18it/s, loss=0.9094]","output_type":"stream"},{"name":"stdout","text":"loss:  0.9093732833862305\nToken Length: 276\n","output_type":"stream"},{"name":"stderr","text":"Training:  55%|█████▌    | 27/49 [00:12<00:09,  2.29it/s, loss=0.8975]","output_type":"stream"},{"name":"stdout","text":"loss:  0.8975078463554382\nToken Length: 251\n","output_type":"stream"},{"name":"stderr","text":"Training:  57%|█████▋    | 28/49 [00:12<00:08,  2.40it/s, loss=0.9586]","output_type":"stream"},{"name":"stdout","text":"loss:  0.9585933685302734\nToken Length: 304\n","output_type":"stream"},{"name":"stderr","text":"Training:  59%|█████▉    | 29/49 [00:12<00:08,  2.41it/s, loss=0.7064]","output_type":"stream"},{"name":"stdout","text":"loss:  0.706447422504425\nToken Length: 242\n","output_type":"stream"},{"name":"stderr","text":"Training:  61%|██████    | 30/49 [00:13<00:07,  2.53it/s, loss=0.7335]","output_type":"stream"},{"name":"stdout","text":"loss:  0.7334791421890259\nToken Length: 304\n","output_type":"stream"},{"name":"stderr","text":"Training:  63%|██████▎   | 31/49 [00:13<00:07,  2.50it/s, loss=0.7026]","output_type":"stream"},{"name":"stdout","text":"loss:  0.7026001811027527\nToken Length: 270\n","output_type":"stream"},{"name":"stderr","text":"Training:  65%|██████▌   | 32/49 [00:14<00:06,  2.47it/s, loss=0.6906]","output_type":"stream"},{"name":"stdout","text":"loss:  0.6905978918075562\nToken Length: 267\n","output_type":"stream"},{"name":"stderr","text":"Training:  67%|██████▋   | 33/49 [00:14<00:06,  2.55it/s, loss=0.5385]","output_type":"stream"},{"name":"stdout","text":"loss:  0.5384976267814636\nToken Length: 270\n","output_type":"stream"},{"name":"stderr","text":"Training:  69%|██████▉   | 34/49 [00:14<00:05,  2.57it/s, loss=0.5258]","output_type":"stream"},{"name":"stdout","text":"loss:  0.5257629156112671\nToken Length: 234\n","output_type":"stream"},{"name":"stderr","text":"Training:  71%|███████▏  | 35/49 [00:15<00:05,  2.70it/s, loss=0.5558]","output_type":"stream"},{"name":"stdout","text":"loss:  0.5557637214660645\nToken Length: 346\n","output_type":"stream"},{"name":"stderr","text":"Training:  73%|███████▎  | 36/49 [00:15<00:05,  2.46it/s, loss=0.5575]","output_type":"stream"},{"name":"stdout","text":"loss:  0.5575355291366577\nToken Length: 290\n","output_type":"stream"},{"name":"stderr","text":"Training:  76%|███████▌  | 37/49 [00:16<00:04,  2.46it/s, loss=0.4267]","output_type":"stream"},{"name":"stdout","text":"loss:  0.4267157316207886\nToken Length: 284\n","output_type":"stream"},{"name":"stderr","text":"Training:  78%|███████▊  | 38/49 [00:16<00:04,  2.51it/s, loss=0.4156]","output_type":"stream"},{"name":"stdout","text":"loss:  0.41561782360076904\nToken Length: 338\n","output_type":"stream"},{"name":"stderr","text":"Training:  80%|███████▉  | 39/49 [00:16<00:04,  2.40it/s, loss=0.4624]","output_type":"stream"},{"name":"stdout","text":"loss:  0.462415874004364\nToken Length: 313\n","output_type":"stream"},{"name":"stderr","text":"Training:  82%|████████▏ | 40/49 [00:17<00:03,  2.32it/s, loss=0.4497]","output_type":"stream"},{"name":"stdout","text":"loss:  0.4497317969799042\nToken Length: 361\n","output_type":"stream"},{"name":"stderr","text":"Training:  84%|████████▎ | 41/49 [00:17<00:03,  2.27it/s, loss=0.3414]","output_type":"stream"},{"name":"stdout","text":"loss:  0.3414402902126312\nToken Length: 323\n","output_type":"stream"},{"name":"stderr","text":"Training:  86%|████████▌ | 42/49 [00:18<00:03,  2.26it/s, loss=0.3164]","output_type":"stream"},{"name":"stdout","text":"loss:  0.3164200484752655\nToken Length: 360\n","output_type":"stream"},{"name":"stderr","text":"Training:  88%|████████▊ | 43/49 [00:18<00:02,  2.21it/s, loss=0.3306]","output_type":"stream"},{"name":"stdout","text":"loss:  0.33059149980545044\nToken Length: 315\n","output_type":"stream"},{"name":"stderr","text":"Training:  90%|████████▉ | 44/49 [00:19<00:02,  2.20it/s, loss=0.3286]","output_type":"stream"},{"name":"stdout","text":"loss:  0.3285524547100067\nToken Length: 343\n","output_type":"stream"},{"name":"stderr","text":"Training:  92%|█████████▏| 45/49 [00:19<00:01,  2.21it/s, loss=0.2760]","output_type":"stream"},{"name":"stdout","text":"loss:  0.276047945022583\nToken Length: 307\n","output_type":"stream"},{"name":"stderr","text":"Training:  94%|█████████▍| 46/49 [00:20<00:01,  2.25it/s, loss=0.2912]","output_type":"stream"},{"name":"stdout","text":"loss:  0.29122108221054077\nToken Length: 313\n","output_type":"stream"},{"name":"stderr","text":"Training:  96%|█████████▌| 47/49 [00:20<00:00,  2.30it/s, loss=0.3467]","output_type":"stream"},{"name":"stdout","text":"loss:  0.34672462940216064\nToken Length: 336\n","output_type":"stream"},{"name":"stderr","text":"Training:  98%|█████████▊| 48/49 [00:20<00:00,  2.21it/s, loss=0.2360]","output_type":"stream"},{"name":"stdout","text":"loss:  0.23596426844596863\nToken Length: 133\n","output_type":"stream"},{"name":"stderr","text":"                                                                      ","output_type":"stream"},{"name":"stdout","text":"loss:  0.19346950948238373\nEpoch 1/1, Training Loss: 1.1742\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}]},{"cell_type":"code","source":"# def evaluate_model(model, dataloader, device):\n#     model.eval()\n#     total_eval_loss = 0\n#     all_preds = []\n#     all_labels = []\n    \n#     progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n\n#     with torch.no_grad():\n#         for batch in progress_bar:\n#             input_ids = batch[\"input_ids\"].to(device[0])\n#             attention_mask = batch[\"attention_mask\"].to(device[0])\n#             token_type_ids = batch[\"token_type_ids\"].to(device[0])\n#             labels = batch[\"label_ids\"].to(device[0])\n\n#             logits = model(input_ids, attention_mask, token_type_ids)\n#             loss, predicted_labels = calculate_loss(logits=logits, labels=labels)\n#             total_eval_loss += loss.item()\n\n#             all_preds.extend(predicted_labels.cpu().numpy().flatten())\n#             all_labels.extend(labels.cpu().numpy().flatten())\n\n#     avg_eval_loss = total_eval_loss / len(dataloader)\n#     print(f\"Validation Loss: {avg_eval_loss:.4f}\")\n\n#     print(\"Classification Report:\")\n#     print(classification_report(all_labels, all_preds, zero_division=0))\n\n\n# # Evaluate the model - train set\n# evaluate_model(model, train_loader, device= Config.torch_device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = PII_Dataset(tokenizer, df=test_df)\n\n# test_loader = DataLoader(\n#     test_dataset,\n#     batch_size=Config.batch_size,\n#     shuffle=True,\n#     collate_fn=Collate(tokenizer),\n#     # num_workers=Config.num_workers,\n#     # pin_memory=True,\n#     drop_last=False,\n# )\n\n# # Evaluate the model - test set\n# evaluate_model(model, test_loader, device= Config.torch_device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.memory._dump_snapshot(\"pytorch_gpu_ram_history.pickle\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}