{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unidecode","metadata":{"_kg_hide-input":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dotenv import load_dotenv\nimport os\nfrom pathlib import Path\nimport pandas as pd\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\nfrom unidecode import unidecode\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom loguru import logger\n\ntorch.cuda.memory._record_memory_history()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_devices() -> list:\n    \"\"\"\n    Returns a list of available torch devices.\n    Prioritizes CUDA (GPU) if available, followed by MPS (Apple Silicon), \n    and defaults to CPU if neither are available.\n    \"\"\"\n    devices = []\n    \n    if torch.cuda.is_available():\n        # Add all available CUDA devices\n        for i in range(torch.cuda.device_count()):\n            device = torch.device(f\"cuda:{i}\")\n            devices.append(device)\n            logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(i)} (cuda:{i})\")\n    \n    elif torch.backends.mps.is_available():\n        # If CUDA is not available, add MPS device (Apple Silicon)\n        device = torch.device(\"mps\")\n        devices.append(device)\n        logger.info(\"Using MPS (Apple Silicon) device.\")\n    \n    else:\n        # If neither CUDA nor MPS are available, default to CPU\n        device = torch.device(\"cpu\")\n        devices.append(device)\n        logger.info(\"Using CPU device.\")\n\n    return devices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config:\n    # Model Config\n    model_id = \"microsoft/deberta-v3-base\"\n    model_architecture_config = AutoConfig.from_pretrained(\n        model_id, output_hidden_states=True\n    )\n\n    # Training Config\n    batch_size = 1\n    max_length = 1024 * 2 + 256\n    num_workers = 2\n\n    # Hardware Config\n    torch_device = get_devices()\n\n    # Dataset\n    dataset_file_path = (\n        \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n    )\n    split_config = {\n        \"test_size\": 0.2,\n        \"shuffle\": True,\n        \"random_state\": 10,\n    }\n    sample_only = False\n    sample_size = 32\n\n    # Labels:\n    label2id = {\n        \"O\": 0,\n        \"B-EMAIL\": 1,\n        \"B-ID_NUM\": 2,\n        \"B-NAME_STUDENT\": 3,\n        \"B-PHONE_NUM\": 4,\n        \"B-STREET_ADDRESS\": 5,\n        \"B-URL_PERSONAL\": 6,\n        \"B-USERNAME\": 7,\n        \"I-ID_NUM\": 8,\n        \"I-NAME_STUDENT\": 9,\n        \"I-PHONE_NUM\": 10,\n        \"I-STREET_ADDRESS\": 11,\n        \"I-URL_PERSONAL\": 12,\n    }\n    id2label = {\n        \"0\": \"O\",\n        \"1\": \"B-EMAIL\",\n        \"2\": \"B-ID_NUM\",\n        \"3\": \"B-NAME_STUDENT\",\n        \"4\": \"B-PHONE_NUM\",\n        \"5\": \"B-STREET_ADDRESS\",\n        \"6\": \"B-URL_PERSONAL\",\n        \"7\": \"B-USERNAME\",\n        \"8\": \"I-ID_NUM\",\n        \"9\": \"I-NAME_STUDENT\",\n        \"10\": \"I-PHONE_NUM\",\n        \"11\": \"I-STREET_ADDRESS\",\n        \"12\": \"I-URL_PERSONAL\",\n    }\n    num_labels = len(label2id)\n\n\nprint(\"torch_device: \", Config.torch_device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    Config.model_id,\n    use_fast=True,  # to avoid warnings\n    clean_up_tokenization_spaces=False,  # to avoid warnings\n    max_length=Config.max_length,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_json(Config.dataset_file_path)\n\nif Config.sample_only:\n    df = df[0 : Config.sample_size]\n\ndf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def replace_space(tokens_list):\n    return [\"[SPACE]\" if x.isspace() else x for x in tokens_list]\n\n\ndef get_tokenized_tokens_length(text):\n    return len(\n        tokenizer(text, return_attention_mask=False, return_token_type_ids=False)[\n            \"input_ids\"\n        ]\n    )\n\n\ndef data_preprocessing(df):\n    df[\"tokens\"] = df[\"tokens\"].apply(replace_space)\n\n    df[\"tokenized_tokens_length\"] = df[\"full_text\"].apply(\n        lambda text: get_tokenized_tokens_length(text)\n    )\n    df = df.sort_values(by=\"tokenized_tokens_length\", ascending=True).reset_index(\n        drop=True\n    )\n\n    return df\n\n\ndf = data_preprocessing(df=df)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df[\"tokenized_tokens_length\"]<256]\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not Config.sample_only:\n    Config.split_config[\"stratify\"] = pd.cut(\n        df[\"tokenized_tokens_length\"], bins=10, labels=False\n    )\n\ntrain_df, test_df = train_test_split(df, **Config.split_config)\ntrain_df.reset_index(inplace=True)\ntrain_df = train_df.sort_values(by=\"tokenized_tokens_length\", ascending=True).reset_index(drop=True)\ntest_df = test_df.sort_values(by=\"tokenized_tokens_length\", ascending=True).reset_index(drop=True)\ntest_df.reset_index(inplace=True)\ntrain_df.shape, test_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_input(row, tokenizer):\n    processed_text_tokens_list = []\n    char_map = []\n    label_char_map = {}\n\n    for index in range(len(row[\"tokens\"])):\n        token = unidecode(row[\"tokens\"][index])\n\n        whitespace = row[\"trailing_whitespace\"][index]\n        label = row[\"labels\"][index]\n\n        processed_text_tokens_list.append(token)\n        char_map.extend([index] * len(token))\n\n        label_char_map[index] = label\n\n        if whitespace:\n            processed_text_tokens_list.append(\" \")\n            char_map.append(-1)\n\n    # Now, we tokenize the concatenated 'text' and return offsets mappings along with 'char_map'.\n    processed_text = \"\".join(processed_text_tokens_list)\n    tokenized = tokenizer(\n        processed_text,\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=Config.max_length,\n    )\n\n    length = len(tokenized.input_ids)\n\n    tokenized_info = {\n        **tokenized,\n        \"processed_text\": processed_text,\n        \"length\": length,\n        \"char_map\": char_map,  # Now includes mapping to original tokens\n        \"label_char_map\": label_char_map,\n    }\n    return tokenized_info\n\n\ndef get_labels(tokenized_info):\n    label_list = []\n    offset_map = tokenized_info[\"offset_mapping\"]\n    for index, offset_map_item in enumerate(offset_map):\n        if offset_map_item == (0, 0):\n            label_list.extend([\"Start_End\"])\n            continue\n\n        char_map_item = tokenized_info[\"char_map\"][\n            offset_map_item[0] : offset_map_item[1]\n        ]\n        char_map_item_filtered = [element for element in char_map_item if element != -1]\n\n        label_item = set(\n            [\n                tokenized_info[\"label_char_map\"][element]\n                for element in char_map_item_filtered\n            ]\n        )\n\n        if len(label_item) != 1:\n            if tokenized_info[\"input_ids\"][index] in [507]:\n                label_item = \"O\"\n\n            else:\n                raise Exception(\n                    \"\\n\"\n                    f\"Token ID: {tokenized_info['input_ids'][index]}\\n\"\n                    f\"Token: {tokenizer.decode(tokenized_info['input_ids'][index])}\\n\"\n                    f\"Offset: {offset_map_item}\\n\"\n                    f\"Text: {tokenized_info['processed_text'][ offset_map_item[0] : offset_map_item[1] ]}\\n\"\n                    f\"Character Map: {char_map_item}\\n\"\n                    f\"Filtered Character Map {char_map_item_filtered}\\n\"\n                    f\"Labels: {label_item}\"\n                )\n\n        label_list.extend(list(label_item))\n\n    if len(label_list) != len(tokenized_info[\"input_ids\"]):\n        raise Exception(\"Error: Size of label_list and input_ids are not same.\")\n    return label_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check - Test - Dataset\nfor index in tqdm(train_df.index):\n    tokenized_info = prepare_input(train_df.iloc[index], tokenizer)\n    label_item = get_labels(tokenized_info)\n\nprint(\"Awesome - Everything is fine\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PII_Dataset(Dataset):\n    def __init__(self, tokenizer, df):\n        self.tokenizer = tokenizer\n        self.df = df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index].to_dict()\n        row.pop(\"tokenized_tokens_length\")\n\n        tokenized_info = prepare_input(self.df.iloc[index], tokenizer)\n        label_item = get_labels(tokenized_info)\n        tokenized_info[\"document_id\"] = row.pop(\"document\")\n        tokenized_info[\"labels\"] = label_item\n        tokenized_info[\"label_ids\"] = [\n            0 if item == \"Start_End\" else Config.label2id[item] for item in label_item\n        ]\n\n        if len(tokenized_info[\"label_ids\"]) != len(tokenized_info[\"input_ids\"]):\n            raise Exception(\n                f\"Error in tokenized_info - length of lavel_ids and input_ids are not same: {tokenized_info}\"\n            )\n        return tokenized_info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        # List of keys to extract from each sample\n        keys = [\n            \"document_id\",\n            \"input_ids\",\n            \"token_type_ids\",\n            \"attention_mask\",\n#             \"offset_mapping\",\n#             \"processed_text\",\n#             \"length\",\n#             \"char_map\",\n#             \"label_char_map\",\n#             \"labels\",\n            \"label_ids\",\n        ]\n\n        # Populate the output dictionary using a loop\n        output = {key: [sample[key] for sample in batch] for key in keys}\n\n        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n\n        # Add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"input_ids\"] = [\n                list(s) + (batch_max - len(s)) * [self.tokenizer.pad_token_id]\n                for s in output[\"input_ids\"]\n            ]\n            output[\"attention_mask\"] = [\n                list(s) + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]\n            ]\n            output[\"token_type_ids\"] = [\n                list(s) + (batch_max - len(s)) * [0] for s in output[\"token_type_ids\"]\n            ]\n#             output[\"offset_mapping\"] = [\n#                 list(s) + (batch_max - len(s)) * [(0, 0)]\n#                 for s in output[\"offset_mapping\"]\n#             ]\n            output[\"label_ids\"] = [\n                list(s) + (batch_max - len(s)) * [0] for s in output[\"label_ids\"]\n            ]\n\n        # Convert to tensors and move to the specified device\n        keys = [\"document_id\", \"input_ids\", \"attention_mask\", \"token_type_ids\", \"label_ids\"]\n        for key in keys:\n            output[key] = torch.tensor(output[key], dtype=torch.long) # .to(Config.torch_device[0])\n\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = PII_Dataset(tokenizer, df=train_df)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=Config.batch_size,\n    shuffle=True,\n    collate_fn=Collate(tokenizer),\n    # num_workers=Config.num_workers,\n    # pin_memory=True,\n    drop_last=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking data loader\nfor item in tqdm(train_loader):\n    if len(item[\"input_ids\"]) != len(item[\"label_ids\"]):\n        raise Exception(\n            \"Error: length of input_ids and label_ids after padding are not same.\"\n        )\n    pass\n\nitem.keys()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PIIDetectionModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.num_labels = Config.model_architecture_config.num_labels\n\n        self.model = AutoModel.from_pretrained(\n            Config.model_id,\n            ignore_mismatched_sizes=True,\n            config=Config.model_architecture_config,\n            # torch_dtype = \"auto\"\n        )\n\n        self.model.resize_token_embeddings(len(tokenizer))\n        self.dropout = torch.nn.Dropout(\n            Config.model_architecture_config.hidden_dropout_prob\n        )\n\n        self.bilstm = torch.nn.LSTM(\n            Config.model_architecture_config.hidden_size,\n            (Config.model_architecture_config.hidden_size) // 2,\n            num_layers=2,\n            dropout=Config.model_architecture_config.hidden_dropout_prob,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.gru = torch.nn.GRU(\n            Config.model_architecture_config.hidden_size,\n            Config.model_architecture_config.hidden_size // 2,\n            num_layers=2,\n            dropout=Config.model_architecture_config.hidden_dropout_prob,\n            batch_first=True,\n            bidirectional=True,\n        )\n        \n        self.lstm_gru_balance_weight = torch.nn.Parameter(\n            torch.tensor(0.5), requires_grad=False\n        )\n\n        self.fc = torch.nn.Linear(\n            Config.model_architecture_config.hidden_size, Config.num_labels\n        )\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )  # returns ['last_hidden_state', 'hidden_states']\n        sequence_output = output[0]\n        sequence_output = self.dropout(sequence_output)\n        lstm_output, hc = self.bilstm(sequence_output)\n        gru_output, _ = self.gru(sequence_output)\n\n        rnn_output = (\n            self.lstm_gru_balance_weight * lstm_output\n            + (1 - self.lstm_gru_balance_weight) * gru_output\n        )\n        print(f\"rnn_output: {rnn_output.shape}\")\n        logits = self.fc(rnn_output)\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_logits_to_labels(logits: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Convert logits into predicted labels for token classification.\n    \"\"\"\n    probabilities = torch.softmax(logits, dim=-1)\n    predicted_labels = torch.argmax(probabilities, dim=-1)\n    return predicted_labels\n\n\ndef calculate_loss(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n    # TODO: Loss function excluding CLS, Start and End tokens: https://chatgpt.com/c/66dbbd83-4930-8007-b247-0d73fc2ee9af\n    \"\"\"\n    Calculate the cross-entropy loss for token classification using raw logits.\n    \"\"\"\n    loss_fn = torch.nn.CrossEntropyLoss()\n    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n    return loss\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = PIIDetectionModel().to(Config.torch_device[0])\nmodel","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef count_trainable_parameters(model: nn.Module) -> int:\n    \"\"\"\n    Count the number of trainable parameters in a PyTorch model.\n    \"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef estimate_parameter_memory(model: nn.Module, dtype=torch.float32) -> float:\n    \"\"\"\n    Estimate the memory required to store the parameters of a PyTorch model. The estimated memory in megabytes (MB).\n    \"\"\"\n    num_params = count_trainable_parameters(model)\n    \n    # Memory per element in bytes, e.g., 4 bytes for float32, 2 bytes for float16\n    bytes_per_element = torch.finfo(dtype).bits // 8\n    \n    # Total memory in bytes\n    total_memory_bytes = num_params * bytes_per_element\n    \n    # Convert to megabytes (MB)\n    total_memory_mb = total_memory_bytes / (1024 ** 2)\n    \n    return num_params, total_memory_mb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_params, total_memory_mb = estimate_parameter_memory(model.model)\nprint(f\"Number of trainable parameters in GRU: {num_params}\")\nprint(f\"Estimated memory for GRU parameters: {total_memory_mb:.3f} MB\\n\")\n\n\nnum_params, total_memory_mb = estimate_parameter_memory(model.bilstm)\nprint(f\"Number of trainable parameters in GRU: {num_params}\")\nprint(f\"Estimated memory for GRU parameters: {total_memory_mb:.3f} MB\\n\")\n\n\nnum_params, total_memory_mb = estimate_parameter_memory(model.gru)\nprint(f\"Number of trainable parameters in GRU: {num_params}\")\nprint(f\"Estimated memory for GRU parameters: {total_memory_mb:.3f} MB\\n\")\n\n\nnum_params, total_memory_mb = estimate_parameter_memory(model.fc)\nprint(f\"Number of trainable parameters in GRU: {num_params}\")\nprint(f\"Estimated memory for GRU parameters: {total_memory_mb:.3f} MB\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Checking data loader\n# for item in tqdm(train_loader):\n#     break\n\n\n# # Chechking models forward pass\n# model_output = model.forward(\n#     input_ids=item[\"input_ids\"].to(Config.torch_device[0]),\n#     attention_mask=item[\"attention_mask\"].to(Config.torch_device[0]),\n#     token_type_ids=item[\"token_type_ids\"].to(Config.torch_device[0]),\n# )\n\n# # from torchviz import make_dot\n# # make_dot(model_output.last_hidden_state.mean(), params=dict(custom_model.named_parameters()))\n\n\n# print(\n#     \"logits_shape: \",\n#     model_output.shape,\n#     \"\\ninput_ids_shape:\",\n#     item[\"input_ids\"].shape,\n#     \"\\noutput_labels_shape: \",\n#     item[\"label_ids\"].shape,\n# )\n\n# # Checking loss function\n# loss, predicted_labels = calculate_loss(logits=model_output, labels=item[\"label_ids\"].to(Config.torch_device[0]))\n# loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.amp import autocast, GradScaler\n\nlearning_rate=5e-5\nepochs=1\n\nscaler = GradScaler() \n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n\n    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n\n    for batch in progress_bar:\n        input_ids = batch[\"input_ids\"].to(Config.torch_device[0])\n        attention_mask = batch[\"attention_mask\"].to(Config.torch_device[0])\n        token_type_ids = batch[\"token_type_ids\"].to(Config.torch_device[0])\n        labels = batch[\"label_ids\"].to(Config.torch_device[0])\n\n        with autocast(device_type=Config.torch_device[0].type):  # Enables mixed precision\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss = calculate_loss(logits=logits, labels=labels)\n\n        scaler.scale(loss).backward()  # Scales loss for better precision\n        scaler.step(optimizer)  # Update model weights\n        scaler.update()\n        \n        optimizer.zero_grad()\n        \n        input_ids.detach()\n        attention_mask.detach()\n        token_type_ids.detach()\n        labels.detach()\n        \n        print(\"loss: \", loss.item())\n\n        total_loss += loss.item()\n        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device):\n    model.eval()\n    total_eval_loss = 0\n    all_preds = []\n    all_labels = []\n    \n    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n\n    with torch.no_grad():\n        for batch in progress_bar:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            token_type_ids = batch[\"token_type_ids\"].to(device)\n            labels = batch[\"label_ids\"].to(device)\n\n            logits = model(input_ids, attention_mask, token_type_ids)\n            loss, predicted_labels = calculate_loss(logits=logits, labels=labels)\n            total_eval_loss += loss.item()\n\n            all_preds.extend(predicted_labels.cpu().numpy().flatten())\n            all_labels.extend(labels.cpu().numpy().flatten())\n\n    avg_eval_loss = total_eval_loss / len(dataloader)\n    print(f\"Validation Loss: {avg_eval_loss:.4f}\")\n\n    print(\"Classification Report:\")\n    print(classification_report(all_labels, all_preds, zero_division=0))\n\n\n# Evaluate the model - train set\nevaluate_model(model, train_loader, device= Config.torch_device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = PII_Dataset(tokenizer, df=test_df)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=Config.batch_size,\n    shuffle=True,\n    collate_fn=Collate(tokenizer),\n    # num_workers=Config.num_workers,\n    # pin_memory=True,\n    drop_last=False,\n)\n\n# Evaluate the model - test set\nevaluate_model(model, test_loader, device= Config.torch_device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.memory._dump_snapshot(\"pytorch_gpu_ram_history.pickle\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}